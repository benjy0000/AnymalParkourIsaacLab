from __future__ import annotations

import torch
from typing import TYPE_CHECKING
from dataclasses import MISSING

from isaaclab.assets import Articulation
from isaaclab.managers import CommandTerm
from isaaclab.managers import CommandTermCfg
from isaaclab.markers import VisualizationMarkers, VisualizationMarkersCfg
from isaaclab.markers.config import RED_ARROW_X_MARKER_CFG
from isaaclab.utils import configclass
from collections.abc import Sequence
from typing import cast

import isaaclab.sim as sim_utils
import isaaclab.utils.math as math_utils

from anymal_parkour.terrains import ParkourTerrainImporter

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


@configclass
class RandomPathCommandCfg(CommandTermCfg):
    """Configuration for the random path command."""
    # The asset to command
    asset_name: str = MISSING
    # The range [min, max] of the radius from the origin to sample the target point.
    target_radius_range: tuple[float, float] = (5.0, 10.0)
    # The proportion of commands that should have speed = 0.0
    rel_standing_env: float = 0.0


@configclass
class RandomSpeedCommandCfg(CommandTermCfg):
    """Configuration for the random speed command."""
    # The asset to command
    asset_name: str = MISSING
    # The range [min, max] of the speed to sample the target speed.
    target_speed_range: tuple[float, float] = (0.0, 1.0)
    # The proportion of commands that should have speed = 0.0
    rel_standing_env: float = 0.0


@configclass
class FollowGoalsCommandCfg(CommandTermCfg):
    """
    Configuration for the follow goals command. The robot will follow a set of predefined
    goals given by the terrain generated by the terrain generator.
    """
    # The asset to command
    asset_name: str = MISSING


class RandomPathCommand(CommandTerm):
    """
    A command generator that produces a fixed, randomly generated target point in the world frame.
    It returns the command vector followed by the next command.

    This command is sampled once per resampling period. The output command vector will
    be the [[x1, y1], [x2, y2]] relative to the origin of the environment.
    """
    cfg: RandomPathCommandCfg

    def __init__(self, cfg: RandomPathCommandCfg, env: ManagerBasedRLEnv):
        super().__init__(cfg, env)
        
        self.robot: Articulation = env.scene[cfg.asset_name]
        self.total_command = torch.zeros(self.num_envs, 2, 2, device=self.device)

    @property
    def command(self) -> torch.Tensor:
        """Returns the command vector."""
        return self.total_command
        
    def _update_metrics(self):
        pass  # No metrics to update for this command
   
    def _resample_command(self, env_ids: Sequence[int] | None = None):
        if env_ids is not None:
            self.total_command[env_ids, 0] = self.total_command[env_ids, 1]
            self.total_command[env_ids, -1] += self._sample_random_target_(len(env_ids))

    def _update_command(self):
        """
        Checks if the robot has reached the current target. If so, it resamples a new command
        for that environment.
        """
        # Get the robot's position relative to its environment's origin
        robot_pos_local = self.robot.data.root_pos_w - self._env.scene.env_origins
        # Calculate the distance to the current target point
        current_goal_pos = self.total_command[:, 0, :]
        distance_to_goal = torch.linalg.vector_norm(robot_pos_local[:, :2] - current_goal_pos, dim=1)
        # Find the environments where the robot is within the 0.1m threshold
        reached_goal_env_ids = torch.where(distance_to_goal < 0.1)[0]
        # If any environments have reached their goal, resample their command
        if reached_goal_env_ids.numel() > 0:
            reached_goal_env_ids_list = reached_goal_env_ids.tolist()
            self._resample_command(reached_goal_env_ids_list)

    def _sample_random_target_(self, size) -> torch.Tensor:
        """
        Generates a new random target point for each environment.
        This is called by the command manager at every resampling interval.
        """
        # Generate a new command vector for each environment
        new_command = torch.zeros((size, 2), device=self.device)

        # Generate random radii, angles and speedsfor each environment
        radii = torch.rand(size, device=self.device) \
            * (self.cfg.target_radius_range[1] - self.cfg.target_radius_range[0]) \
            + self.cfg.target_radius_range[0]

        angles = torch.rand(size, device=self.device) * 2 * torch.pi

        # Convert polar coordinates to Cartesian to get the target points in the world frame
        target_x = radii * torch.cos(angles)
        target_y = radii * torch.sin(angles)

        # Store the target point in the command vector
        new_command[:, 0] = target_x
        new_command[:, 1] = target_y

        return new_command

    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:
        """Resets the command for the specified environments."""
        # Reset the command vector to zeros for the specified environments
        # Flushes out the zeros by resampling one extra time
        if env_ids is not None:
            self.total_command[env_ids] = torch.zeros(len(env_ids), 2, 2, device=self.device)
            self._resample_command(env_ids)
        return super().reset(env_ids)
    

class RandomSpeedCommand(CommandTerm):
    """
    A command generator that produces a random speed to be followed.
    """
    cfg: RandomSpeedCommandCfg

    def __init__(self, cfg: RandomSpeedCommandCfg, env: ManagerBasedRLEnv):
        super().__init__(cfg, env)
        
        self.robot: Articulation = env.scene[cfg.asset_name]
        self.speed_command: torch.Tensor = torch.zeros(self.num_envs, device=self.device)

    @property
    def command(self) -> torch.Tensor:
        """Returns the command vector."""
        return self.speed_command

    def _update_metrics(self):
        pass  # No metrics to update for this command
   
    def _resample_command(self, env_ids: Sequence[int] | None = None):
        if env_ids is not None:
            speeds = (torch.rand(len(env_ids), device=self.device)
                      * (self.cfg.target_speed_range[1] - self.cfg.target_speed_range[0])
                      + self.cfg.target_speed_range[0])
            self.speed_command[env_ids] = speeds

    def _update_command(self):
        pass

    def _set_debug_vis_impl(self, debug_vis: bool):
        pass

    def _debug_vis_callback(self, event):
        pass


class FollowGoalsCommand(CommandTerm):
    """
    A command that follows a set of goals. Dependent on the terrain remaining constant
    for each robot (ignoring difficulty).

    Returns a tensor similar to that of RandomPathCommand, but this time it is the goal positions.
    The command is relative to the environment origin.
    If the goal is the last one then it will be repeated.
    """

    cfg: FollowGoalsCommandCfg

    def __init__(self, cfg: FollowGoalsCommandCfg, env: ManagerBasedRLEnv):
        super().__init__(cfg, env)

        self.env = env
        self.robot: Articulation = env.scene[cfg.asset_name]
        self.total_command = torch.zeros(self.num_envs, 2, 2, device=self.device)

        # Fetch goals from the terrain
        self.terrain: ParkourTerrainImporter = cast(ParkourTerrainImporter, env.scene.terrain)
        envs = torch.arange(self.num_envs, device=self.device)
        self.goals = self.terrain.fetch_goals_from_env(envs)

        # Store the number of goals reached for each environment
        self.num_goals_reached = torch.zeros(self.num_envs, dtype=torch.int, device=self.device)
        self.num_goals = self.goals.shape[1]

    @property
    def command(self) -> torch.Tensor:
        """Returns the command vector."""
        return self.total_command
        
    def _update_metrics(self):
        pass  # No metrics to update for this command
   
    def _resample_command(self, env_ids: Sequence[int] | None = None):
        if env_ids is not None:
            # first convert env_ids to a tensor
            tensor_env_ids = torch.as_tensor(env_ids, dtype=torch.long, device=self.device)
            # Find relevant ids
            first_command_ids = tensor_env_ids[torch.where(self.num_goals_reached[tensor_env_ids] == 0)[0]]
            other_command_ids = tensor_env_ids[torch.where(self.num_goals_reached[tensor_env_ids] > 0)[0]]
            # Update those that have just been reset
            if first_command_ids.numel() > 0:
                self.total_command[first_command_ids, 0, :] = self.goals[first_command_ids, 0, :2]
                self.total_command[first_command_ids, 1, :] = self.goals[first_command_ids, 1, :2]
            # Update others and make sure they don't go out of bounds
            if other_command_ids.numel() > 0:
                self.total_command[other_command_ids, 0, :] = self.goals[
                    other_command_ids,
                    torch.clamp(self.num_goals_reached[other_command_ids], max=self.num_goals - 1),
                    :2
                ]
                self.total_command[other_command_ids, 1, :] = self.goals[
                    other_command_ids,
                    torch.clamp(self.num_goals_reached[other_command_ids] + 1, max=self.num_goals - 1),
                    :2
                ]

    def _update_command(self):
        """
        Checks if the robot has reached the current target. If so, it updates the command.
        """
        # Get the robot's position relative to its environment's origin
        robot_pos_local = self.robot.data.root_pos_w - self._env.scene.env_origins
        # Calculate the distance to the current target point
        current_goal_pos = self.total_command[:, 0, :]
        distance_to_goal = torch.linalg.vector_norm(robot_pos_local[:, :2] - current_goal_pos, dim=1)
        # Find the environments where the robot is within the 0.1m threshold
        reached_goal_env_ids = torch.where(distance_to_goal < 0.1)[0]
        # If any environments have reached their goal, resample their command
        if reached_goal_env_ids.numel() > 0:
            # Update the number of goals reached
            self.num_goals_reached[reached_goal_env_ids] += 1
            # Reset the environments that have reached their last goal
            reached_goal_env_ids_list = reached_goal_env_ids.tolist()
            self._resample_command(reached_goal_env_ids_list)

    def reset(self, env_ids: Sequence[int] | None = None) -> dict[str, float]:
        """Resets the command for the specified environments."""
        # Set the number of goals reached to zero
        if env_ids is not None:
            self.num_goals_reached[env_ids] = 0
            # reset goals to match new enviroments
            self.goals[env_ids, :, :] = self.terrain.fetch_goals_from_env(torch.tensor(env_ids, dtype=torch.int64, device=self.device))
        return super().reset(env_ids)
    
    def _set_debug_vis_impl(self, debug_vis: bool):
        # set visibility of markers 
        # note: parent only deals with callbacks. not their visibility
        if debug_vis:
            # create markers if necessary for the first time
            if not hasattr(self, "robot_goal_visualizer"):
                # -- goal
                goal_direction_visualizer_cfg: VisualizationMarkersCfg = RED_ARROW_X_MARKER_CFG.replace(
                    prim_path="/Visuals/Command/velocity_goal"
                )
                self.robot_goal_visualizer = VisualizationMarkers(goal_direction_visualizer_cfg)
                # -- current
                robot_target_visualizer_cfg = VisualizationMarkersCfg(
                    prim_path="/Visuals/TerrainGoals",
                    markers={
                        "marker1": sim_utils.SphereCfg(
                            radius=0.05,
                            visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)))
                    }
                )
                self.robot_target_visualizer = VisualizationMarkers(robot_target_visualizer_cfg)
            # set their visibility to true
            self.robot_goal_visualizer.set_visibility(True)
            self.robot_target_visualizer.set_visibility(True)
        else:
            if hasattr(self, "robot_goal_visualizer"):
                self.robot_goal_visualizer.set_visibility(False)
                self.robot_target_visualizer.set_visibility(False)

    def _debug_vis_callback(self, event):
        # check if robot is initialized
        # note: this is needed in-case the robot is de-initialized. we can't access the data
        if not self.robot.is_initialized:
            return
        # get marker location
        # -- base state
        base_pos_w = self.robot.data.root_pos_w.clone()
        base_pos_w[:, 2] += 0.5
        # -- resolve the locations and quaternions
        goal_arrow_quat = self._get_yaw_quat_from_target()
        goal_location = self._get_next_goal_position()

        # display markers
        self.robot_goal_visualizer.visualize(base_pos_w, goal_arrow_quat)
        self.robot_target_visualizer.visualize(goal_location)

    def _get_yaw_quat_from_target(self) -> torch.Tensor:
        # Get the robot's current XY position relative to its environment origin
        robot_pos_local = self.robot.data.root_pos_w - self.env.scene.env_origins
        robot_xy_local = robot_pos_local[:, :2]
        # Get the current target point from the command manager
        target_xy_local = self.total_command[:, 0, :]
        # Calculate the desired yaw to face the target point
        direction_vector = target_xy_local - robot_xy_local
        desired_yaw = torch.atan2(direction_vector[..., 1], direction_vector[..., 0])
        zeros = torch.zeros_like(desired_yaw)
        arrow_quat_b = math_utils.quat_from_euler_xyz(zeros, zeros, desired_yaw)
        # convert everything back from base to world frame
        base_quat_w = self.robot.data.root_quat_w
        arrow_quat = math_utils.quat_mul(base_quat_w, arrow_quat_b)

        return arrow_quat

    def _get_next_goal_position(self) -> torch.Tensor:
        # Get the next goal position from the command manager

        position = torch.zeros(self.env.num_envs, 3)
        position[:, :2] = self.total_command[:, 0, :].squeeze()
        return position + self.env.scene.env_origins
